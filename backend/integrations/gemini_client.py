"""
Gemini AI client for CodegenCICD Dashboard
"""
from typing import Dict, Any, Optional, List
import structlog

from .base_client import BaseClient, APIError
from backend.config import get_settings

logger = structlog.get_logger(__name__)
settings = get_settings()


class GeminiClient(BaseClient):
    """Client for interacting with Google Gemini API"""
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or settings.gemini_api_key
        
        if not self.api_key:
            raise ValueError("Gemini API key is required")
        
        super().__init__(
            service_name="gemini_api",
            base_url="https://generativelanguage.googleapis.com/v1beta",
            api_key=self.api_key,
            timeout=60,  # Longer timeout for AI responses
            max_retries=3
        )
    
    def _get_default_headers(self) -> Dict[str, str]:
        """Get default headers for Gemini API requests"""
        return {
            "Content-Type": "application/json",
            "User-Agent": "CodegenCICD-Dashboard/1.0"
        }
    
    async def _health_check_request(self) -> None:
        """Health check by listing models"""
        await self.get(f"/models?key={self.api_key}")
    
    async def generate_content(self,
                              prompt: str,
                              model: str = "gemini-1.5-flash",
                              temperature: float = 0.1,
                              max_output_tokens: int = 2048) -> Dict[str, Any]:
        """Generate content using Gemini"""
        try:
            payload = {
                "contents": [{
                    "parts": [{
                        "text": prompt
                    }]
                }],
                "generationConfig": {
                    "temperature": temperature,
                    "maxOutputTokens": max_output_tokens,
                    "topP": 0.8,
                    "topK": 10
                }
            }
            
            self.logger.info("Generating content with Gemini",
                           model=model,
                           prompt_length=len(prompt),
                           temperature=temperature)
            
            response = await self.post(
                f"/models/{model}:generateContent?key={self.api_key}",
                data=payload
            )
            
            # Extract the generated text
            candidates = response.get("candidates", [])
            if candidates and candidates[0].get("content", {}).get("parts"):
                generated_text = candidates[0]["content"]["parts"][0].get("text", "")
                
                self.logger.info("Content generated successfully",
                               response_length=len(generated_text),
                               model=model)
                
                return {
                    "text": generated_text,
                    "model": model,
                    "usage": response.get("usageMetadata", {}),
                    "raw_response": response
                }
            else:
                raise APIError("No content generated by Gemini")
            
        except Exception as e:
            self.logger.error("Failed to generate content",
                            model=model,
                            error=str(e))
            raise
    
    # Validation-specific methods
    async def analyze_deployment_logs(self,
                                    logs: str,
                                    project_context: str = "") -> Dict[str, Any]:
        """Analyze deployment logs for success/failure and confidence score"""
        try:
            prompt = f"""
Analyze the following deployment logs and provide a structured assessment:

Project Context: {project_context}

Deployment Logs:
{logs}

Please provide your analysis in the following JSON format:
{{
    "status": "success" or "failure",
    "confidence_score": <number between 0-100>,
    "summary": "<brief summary of deployment status>",
    "issues_found": ["<list of issues if any>"],
    "recommendations": ["<list of recommendations>"],
    "key_indicators": {{
        "success_indicators": ["<indicators of success>"],
        "failure_indicators": ["<indicators of failure>"]
    }}
}}

Focus on:
1. Exit codes and error messages
2. Service startup success
3. Database connections
4. Port binding and network issues
5. Dependency installation
6. Build process completion
"""
            
            response = await self.generate_content(
                prompt=prompt,
                temperature=0.1,  # Low temperature for consistent analysis
                max_output_tokens=1024
            )
            
            # Try to parse JSON from response
            import json
            try:
                analysis = json.loads(response["text"])
                analysis["raw_analysis"] = response["text"]
                return analysis
            except json.JSONDecodeError:
                # If JSON parsing fails, return structured response
                return {
                    "status": "unknown",
                    "confidence_score": 50,
                    "summary": "Could not parse deployment analysis",
                    "raw_analysis": response["text"],
                    "issues_found": ["Analysis parsing failed"],
                    "recommendations": ["Manual review required"]
                }
            
        except Exception as e:
            self.logger.error("Failed to analyze deployment logs", error=str(e))
            raise
    
    async def analyze_code_quality(self,
                                  code_analysis_results: Dict[str, Any],
                                  project_context: str = "") -> Dict[str, Any]:
        """Analyze code quality results from graph-sitter"""
        try:
            prompt = f"""
Analyze the following code quality results and provide a structured assessment:

Project Context: {project_context}

Code Analysis Results:
{str(code_analysis_results)}

Please provide your analysis in the following JSON format:
{{
    "overall_score": <number between 0-100>,
    "quality_grade": "<A/B/C/D/F>",
    "summary": "<brief summary of code quality>",
    "strengths": ["<list of code strengths>"],
    "weaknesses": ["<list of code weaknesses>"],
    "recommendations": ["<list of improvement recommendations>"],
    "metrics": {{
        "complexity_score": <0-100>,
        "maintainability_score": <0-100>,
        "test_coverage_score": <0-100>,
        "documentation_score": <0-100>
    }}
}}

Focus on:
1. Code complexity and maintainability
2. Test coverage and quality
3. Documentation completeness
4. Security considerations
5. Performance implications
6. Best practices adherence
"""
            
            response = await self.generate_content(
                prompt=prompt,
                temperature=0.1,
                max_output_tokens=1024
            )
            
            # Try to parse JSON from response
            import json
            try:
                analysis = json.loads(response["text"])
                analysis["raw_analysis"] = response["text"]
                return analysis
            except json.JSONDecodeError:
                return {
                    "overall_score": 75,  # Default reasonable score
                    "quality_grade": "B",
                    "summary": "Code quality analysis completed",
                    "raw_analysis": response["text"],
                    "strengths": ["Analysis completed"],
                    "weaknesses": ["Analysis parsing failed"],
                    "recommendations": ["Manual review recommended"]
                }
            
        except Exception as e:
            self.logger.error("Failed to analyze code quality", error=str(e))
            raise
    
    async def analyze_ui_test_results(self,
                                    test_results: Dict[str, Any],
                                    project_context: str = "") -> Dict[str, Any]:
        """Analyze UI test results from web-eval-agent"""
        try:
            prompt = f"""
Analyze the following UI test results and provide a structured assessment:

Project Context: {project_context}

UI Test Results:
{str(test_results)}

Please provide your analysis in the following JSON format:
{{
    "overall_score": <number between 0-100>,
    "test_status": "passed" or "failed" or "partial",
    "summary": "<brief summary of UI test results>",
    "passed_tests": <number>,
    "failed_tests": <number>,
    "critical_failures": ["<list of critical UI failures>"],
    "minor_issues": ["<list of minor UI issues>"],
    "recommendations": ["<list of UI improvement recommendations>"],
    "user_experience_score": <0-100>
}}

Focus on:
1. Functional test results
2. User interface responsiveness
3. Accessibility compliance
4. Cross-browser compatibility
5. Performance metrics
6. User experience quality
"""
            
            response = await self.generate_content(
                prompt=prompt,
                temperature=0.1,
                max_output_tokens=1024
            )
            
            # Try to parse JSON from response
            import json
            try:
                analysis = json.loads(response["text"])
                analysis["raw_analysis"] = response["text"]
                return analysis
            except json.JSONDecodeError:
                return {
                    "overall_score": 80,  # Default reasonable score
                    "test_status": "partial",
                    "summary": "UI test analysis completed",
                    "raw_analysis": response["text"],
                    "passed_tests": 0,
                    "failed_tests": 0,
                    "critical_failures": ["Analysis parsing failed"],
                    "recommendations": ["Manual UI review recommended"]
                }
            
        except Exception as e:
            self.logger.error("Failed to analyze UI test results", error=str(e))
            raise
    
    async def generate_error_fix_suggestions(self,
                                           error_context: str,
                                           code_context: str = "",
                                           project_context: str = "") -> Dict[str, Any]:
        """Generate suggestions for fixing errors"""
        try:
            prompt = f"""
Analyze the following error and provide structured fix suggestions:

Project Context: {project_context}

Code Context: {code_context}

Error Details:
{error_context}

Please provide your suggestions in the following JSON format:
{{
    "error_type": "<classification of error>",
    "severity": "low" or "medium" or "high" or "critical",
    "root_cause": "<likely root cause of the error>",
    "fix_suggestions": [
        {{
            "approach": "<fix approach name>",
            "description": "<detailed description>",
            "confidence": <0-100>,
            "effort": "low" or "medium" or "high"
        }}
    ],
    "prevention_tips": ["<tips to prevent similar errors>"],
    "related_documentation": ["<relevant documentation links or topics>"]
}}

Focus on:
1. Identifying the root cause
2. Providing actionable fix suggestions
3. Estimating fix complexity and confidence
4. Suggesting prevention strategies
5. Recommending relevant documentation
"""
            
            response = await self.generate_content(
                prompt=prompt,
                temperature=0.2,  # Slightly higher for creative solutions
                max_output_tokens=1536
            )
            
            # Try to parse JSON from response
            import json
            try:
                suggestions = json.loads(response["text"])
                suggestions["raw_suggestions"] = response["text"]
                return suggestions
            except json.JSONDecodeError:
                return {
                    "error_type": "unknown",
                    "severity": "medium",
                    "root_cause": "Unable to analyze error",
                    "raw_suggestions": response["text"],
                    "fix_suggestions": [{
                        "approach": "Manual Investigation",
                        "description": "Manual investigation required due to analysis parsing failure",
                        "confidence": 30,
                        "effort": "high"
                    }],
                    "prevention_tips": ["Improve error logging and monitoring"]
                }
            
        except Exception as e:
            self.logger.error("Failed to generate error fix suggestions", error=str(e))
            raise
    
    async def list_models(self) -> List[Dict[str, Any]]:
        """List available Gemini models"""
        try:
            response = await self.get(f"/models?key={self.api_key}")
            return response.get("models", [])
        except Exception as e:
            self.logger.error("Failed to list models", error=str(e))
            raise

